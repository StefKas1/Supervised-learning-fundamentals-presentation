{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Move from Data to (Machine Learning) Models to Decisions?\n",
    "\n",
    "No prior knowledge needed: Learn how to move from data to (machine learning) models to decisions. Learn what terms such as (machine learning) models, supervised learning, unsupervised learning, regression, classification, clustering and more mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` directory\n",
    "Artificial Intelligence (AI)\n",
    "└── Machine Learning (ML)\n",
    "    ├── Supervised Learning\n",
    "    ├── Unsupervised Learning\n",
    "    ├── Semi-Supervised Learning\n",
    "    ├── Reinforcement Learning\n",
    "    ├── Deep Learning\n",
    "    └── Generative AI\n",
    "```\n",
    "\n",
    "- Artificial Intelligence (AI): Creating machines that can perform tasks that typically require human intelligence / that replicate human behavior.\n",
    "- Machine Learning (ML): A subset of AI where machines learn relationships hidden in data - to model real systems.\n",
    "- Deep Learning: A branch of ML that uses neural networks with many layers and neurons - to model real systems.\n",
    "- Generative AI: Often using deep learning / neural networks with 100's of billions of neurons that creates new content, like text, images, or music, by learning patterns from existing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised: regression\n",
    "<img src=\"images/01_regression_income_2.PNG\" alt=\"Regression Income\" width=\"1200\"/>\n",
    "\n",
    "- `age` alone is unlikely to provide an accurate prediction of a particular man’s `wage`.\n",
    "- Number of features might be quite large, such as on the order of thousands or even millions.\n",
    "- Regression: predicting a continuous output value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised: classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/02_classification_churn.PNG\" alt=\"Classification Churn\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised: clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Clustering Gene](images/03_clustering_gene.PNG)\n",
    "- $Z_1$ and $Z_2$ are the first two principal components of the data, which summarize the data down from 64 features to two numbers or dimensions.\n",
    "- Likely dimension reduction has resulted in some loss of information, but it is now possible to visually examine the data for evidence of clustering.\n",
    "- Cell lines with same cancer type tend to be located near each other in this two-dimensional representation; even though the cancer information was not used to produce the left-hand panel, the clustering obtained does bear some resemblance to some of the actual cancer types observed in the right-hand panel.\n",
    "- Clustering: grouping according to observed characteristics (no labels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised learning: regression - in depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observed data\n",
    "| $(i)$ | Education $({x}_i)$ | Income $({y}_i)$ |\n",
    "|-------|---------------------|------------------|\n",
    "| 1 | 10          | 26.6588387834389 |\n",
    "| 2 | 10.40133779 | 27.3064353457772 |\n",
    "| 3 | 10.84280936 | 22.1324101716143 |\n",
    "| 4 | 11.24414716 | 21.1698405046065 |\n",
    "| 5 | 11.64548495 | 15.1926335164307 |\n",
    "| 6 | 12.08695652 | 26.3989510407284 |\n",
    "| 7 | 12.48829431 | 17.435306578572  |\n",
    "| 8 | 12.88963211 | 25.5078852305278 |\n",
    "| 9 | 13.2909699  | 36.884594694235  |\n",
    "| 10| 13.73244147 | 39.666108747637  |\n",
    "| 11| 14.13377926 | 34.3962805641312 |\n",
    "| 12| 14.53511706 | 41.4979935356871 |\n",
    "| 13| 14.97658863 | 44.9815748660704 |\n",
    "| 14| 15.37792642 | 47.039595257834  |\n",
    "| 15| 15.77926421 | 48.2525782901863 |\n",
    "| 16| 16.22073579 | 57.0342513373801 |\n",
    "| 17| 16.62207358 | 51.4909192102538 |\n",
    "| 18| 17.02341137 | 61.3366205527288 |\n",
    "| 19| 17.46488294 | 57.581988179306  |\n",
    "| 20| 17.86622074 | 68.5537140185881 |\n",
    "| 21| 18.26755853 | 64.310925303692  |\n",
    "| 22| 18.7090301  | 68.9590086393083 |\n",
    "| 23| 19.11036789 | 74.6146392793647 |\n",
    "| 24| 19.51170569 | 71.8671953042483 |\n",
    "| 25| 19.91304348 | 76.098135379724  |\n",
    "| 26| 20.35451505 | 75.77521802986   |\n",
    "| 27| 20.75585284 | 72.4860553152424 |\n",
    "| 28| 21.15719064 | 77.3550205741877 |\n",
    "| 29| 21.59866221 | 72.1187904524136 |\n",
    "| 30| 22          | 80.2605705009016 |\n",
    "\n",
    "<br>\n",
    "<img src=\"images/04_regression_education_only_data_points.PNG\" alt=\"Regression Education Only Data\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "- $ x_i $ is the observed value (actual value) for the $ i $-th observation.\n",
    "- $ x_i $ is the independent variable (input / feature / predictor) for the $ i $-th observation.\n",
    "- $ y_i $ is the observed value (actual value) for the $ i $-th observation.\n",
    "- $ y_i $ is the dependent variable (output / label / response) for the $ i $-th observation.\n",
    "\n",
    "\n",
    "\n",
    "### 1. **Simple Linear Regression Model**\n",
    "\n",
    "<img src=\"images/04_regression_education_with_two_models.PNG\" alt=\"Regression Education With Models\" width=\"600\"/>\n",
    "\n",
    "Problem:\n",
    "- The true relationship between $ x_i $ and $ y_i $ across all observations is typically complex and influenced by numerous factors. Therefore, an exact model that describes the relationship between $\\mathbf{x}$ and $\\mathbf{y}$ is typically not attainable.\n",
    "\n",
    "Solution:\n",
    "- Try to find a model that approximates $ y $.\n",
    "- The the model $$ \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i $$ provides a simplified approximation: $ \\hat{y} \\approx y $.\n",
    "\n",
    "Where:\n",
    "- $ \\hat{\\beta}_0 $ is the estimated intercept (the estimated value of $ \\hat{y} $ when $ x = 0 $).\n",
    "- $ \\hat{\\beta}_1 $ is the estimated slope (the estimated change in $ \\hat{y} $ for a one-unit change in $ x $).\n",
    "- $ \\hat{\\mathbf{\\beta}} $ is the vector that represents the estimated coefficients / parameters / effects.\n",
    "- $ \\hat{y}_i $ is the predicted value of $ y $ for the $ i $-th observation, calculated using the estimated model.\n",
    "- $ \\hat{y}_i $ represents the part of $ y_i $ that is explained by the linear relationship with $ x_i $ according to the estimated model.\n",
    "- Estimated model equation represents the relationship between $ x_i $ and $ \\hat{y}_i $ for the $ i $-th observation, based on the estimated coefficients.\n",
    "- Estimated model equation describes how the dependent variable ( $ \\hat{y} $ ) depends on one or more independent variables ( $ x $ ) based on the data.\n",
    "- Estimated model is the equation: $ \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i $.\n",
    "- Estimated model expresses that each predicted $ \\hat{y}_i $ is composed of the estimated systematic part ($ \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i $), which represents the linear relationship with $ x_i $ as derived from the observed data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2. **Mean Squared Error (MSE)**\n",
    "\n",
    "<img src=\"images/05_regression_education_residuals.PNG\" alt=\"Regression Education With Residuals\" width=\"600\"/>\n",
    "\n",
    "The Mean Squared Error (MSE) is a measure of how well the model's predictions $ \\hat{y}_i $ match the actual observations $ y_i $. It is defined as:\n",
    "\n",
    "$$\n",
    "\\text{MSE}(\\hat{\\beta}_0, \\hat{\\beta}_1) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2\n",
    "$$\n",
    "\n",
    "Where the predicted value $ \\hat{y}_i $ is given by:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\n",
    "$$\n",
    "\n",
    "Substituting $ \\hat{y}_i $ into the MSE formula:\n",
    "\n",
    "$$\n",
    "\\text{MSE}(\\hat{\\beta}_0, \\hat{\\beta}_1) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) \\right)^2\n",
    "$$\n",
    "\n",
    "### 3. **Gradient Descent**\n",
    "\n",
    "To minimize the MSE, we use the Gradient Descent algorithm. This involves updating the parameters $ \\hat{\\beta}_0 $ and $ \\hat{\\beta}_1 $ iteratively in the direction of the negative gradient of the MSE.\n",
    "\n",
    "#### 3.1. **Compute the Gradients**\n",
    "\n",
    "The partial derivatives of the MSE with respect to $ \\hat{\\beta}_0 $ and $ \\hat{\\beta}_1 $ are:\n",
    "\n",
    "1. **Gradient with respect to $ \\hat{\\beta}_0 $** (Intercept):\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial \\text{MSE}}{\\partial \\hat{\\beta}_0} = -\\frac{2}{n} \\sum_{i=1}^{n} \\left( y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) \\right)\n",
    "   $$\n",
    "\n",
    "2. **Gradient with respect to $ \\hat{\\beta}_1 $** (Slope):\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial \\text{MSE}}{\\partial \\hat{\\beta}_1} = -\\frac{2}{n} \\sum_{i=1}^{n} \\left( y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) \\right) x_i\n",
    "   $$\n",
    "\n",
    "#### 3.2. **Update Rules**\n",
    "\n",
    "Using the gradients, we update $ \\hat{\\beta}_0 $ and $ \\hat{\\beta}_1 $ using the following rules:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_0^{(k+1)} = \\hat{\\beta}_0^{(k)} - \\alpha \\frac{\\partial \\text{MSE}}{\\partial \\hat{\\beta}_0}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_1^{(k+1)} = \\hat{\\beta}_1^{(k)} - \\alpha \\frac{\\partial \\text{MSE}}{\\partial \\hat{\\beta}_1}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\hat{\\beta}_0^{(k)} $ and $ \\hat{\\beta}_1^{(k)} $ are the estimates of the parameters at the $ k $-th iteration.\n",
    "- $ \\alpha $ is the learning rate, which controls the step size of each update.\n",
    "\n",
    "### 4. **Convergence and Final Model**\n",
    "\n",
    "After many iterations, the algorithm will converge to the final estimates $ \\hat{\\beta}_0 $ and $ \\hat{\\beta}_1 $, which minimize the MSE and represent the best-fit line for the data.\n",
    "\n",
    "The final model can then be used for making predictions:\n",
    "\n",
    "$$\n",
    "\\hat{y} = -39.5 + 5.6 x\n",
    "$$\n",
    "\n",
    "<img src=\"images/06_regression_education_residuals_mse_surface.PNG\" alt=\"MSE Surface\" width=\"800\"/>\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"images/06_1_convex_and_non_convex_surface.PNG\" alt=\"MSE Surface\"/>\n",
    "Left convex function, right non-convex function.\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"images/06_2_non_convex_surface.PNG\" alt=\"MSE Surface\" width=\"1200\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. **Beyond Simple Linear Regression Models**\n",
    "\n",
    "<img src=\"images/07_regression_education_seniority_eg_multiple.PNG\" alt=\"2 Independent Variables\" width=\"800\"/> <br>\n",
    "\n",
    "$$ \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{1i} + \\hat{\\beta}_2 x_{2i} $$\n",
    "\n",
    "\n",
    "<img src=\"images/08_regression_education_seniority_eg_polynomial.PNG\" alt=\"2 Independent Variables\" width=\"800\"/>\n",
    "\n",
    "\n",
    "$$ \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{1i} + \\hat{\\beta}_2 x_{2i} + \\hat{\\beta}_3 x_{1i}^2 + \\hat{\\beta}_4 x_{2i}^2 + \\hat{\\beta}_5 x_{1i}x_{2i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear Regression\n",
    "- Polynomial Regression\n",
    "- Ridge Regression\n",
    "- Lasso Regression\n",
    "- Elastic Net Regression\n",
    "- Neural Network Regression\n",
    "- Decision Tree Regression\n",
    "- Random Forest Regression\n",
    "- Support Vector Regression\n",
    "- K-Nearest Neighbors Regression\n",
    "- Combining Regression models\n",
    "- ...\n",
    "\n",
    "### Decision Tree (Regression) & Random Forest (Regression)\n",
    "<img src=\"images/08_01_regression_decision_tree.PNG\" alt=\"Regression Decision Tree\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised learning: classification - in depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observed data\n",
    "| Default | Balance  |\n",
    "|---------|----------|\n",
    "| No      | 729.526  |\n",
    "| No      | 817.180  |\n",
    "| No      | 1073.549 |\n",
    "| Yes     | 1838.871 |\n",
    "| Yes     | 1893.023 |\n",
    "| Yes     | 1605.214 |\n",
    "| ...     | ...      |\n",
    "\n",
    "<br>\n",
    "<img src=\"images/09_classification.PNG\" alt=\"Classification\"/>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"images/10_linear_regression_and_logistic_regression.PNG\" alt=\"Linear Regression and Logistic Regression\"/>\n",
    "\n",
    "Linear regression on the left, logistic regression on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a classification problem with one independent variable, we typically use **logistic regression** rather than linear regression. Logistic regression is used to model the probability that a given observation belongs to a particular class (usually binary, such as 0 or 1).\n",
    "\n",
    "### 1. **Logistic Regression Model**\n",
    "\n",
    "In logistic regression, the relationship between the dependent variable $ y $ (which is binary) and the independent variable $ x $ is modeled using the logistic function (also called the sigmoid function):\n",
    "\n",
    "$$\n",
    "\\hat{p}(x) = \\frac{1}{1 + e^{-(\\hat{\\beta}_0 + \\hat{\\beta}_1 x)}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\hat{p}(x) $ is the estimated probability that the dependent variable $ y = 1 $ given the independent variable $ x $.\n",
    "- $ \\hat{\\beta}_0 $ is the estimated intercept.\n",
    "- $ \\hat{\\beta}_1 $ is the estimated coefficient (slope) for the independent variable $ x $.\n",
    "\n",
    "The estimated probability $ \\hat{p}(x) $ is then used to classify the observation:\n",
    "- If $ \\hat{p}(x) > 0.5 $, predict $ y = 1 $.\n",
    "- If $ \\hat{p}(x) \\leq 0.5 $, predict $ y = 0 $.\n",
    "\n",
    "\n",
    "### 2. **Log-Loss (Binary Cross-Entropy Loss)**\n",
    "### 2.1 **Likelihood Function**\n",
    "\n",
    "In maximum likelihood estimation (MLE), we aim to find the parameters $ \\hat{\\beta}_0 $ and $ \\hat{\\beta}_1 $ that maximize the likelihood of observing the given data.\n",
    "\n",
    "The likelihood for a single observation $ (x_i, y_i) $ is:\n",
    "\n",
    "$$\n",
    "\\text{Likelihood} = \\hat{p}(y_i | x_i) = \\hat{p}(x_i)^{y_i} \\cdot (1 - \\hat{p}(x_i))^{1 - y_i}\n",
    "$$\n",
    "\n",
    "This likelihood function works for both cases:\n",
    "- If $ y_i = 1 $, then the likelihood is $ \\hat{p}(x_i) $.\n",
    "- If $ y_i = 0 $, then the likelihood is $ 1 - \\hat{p}(x_i) $.\n",
    "\n",
    "### 2.2 **Log-Likelihood Function**\n",
    "\n",
    "To make the optimization easier, we typically work with the **log-likelihood** instead of the likelihood. The log-likelihood for a single observation is the natural logarithm of the likelihood:\n",
    "\n",
    "$$\n",
    "\\text{Log-Likelihood} = \\log(\\text{Likelihood}) = \\log\\left( \\hat{p}(x_i)^{y_i} \\cdot (1 - \\hat{p}(x_i))^{1 - y_i} \\right)\n",
    "$$\n",
    "\n",
    "Using the logarithm property $ \\log(a \\cdot b) = \\log(a) + \\log(b) $, we can rewrite the log-likelihood as:\n",
    "\n",
    "$$\n",
    "\\text{Log-Likelihood} = y_i \\log(\\hat{p}(x_i)) + (1 - y_i) \\log(1 - \\hat{p}(x_i))\n",
    "$$\n",
    "\n",
    "### 2.3 **(Total) Log-Likelihood for All Observations**\n",
    "\n",
    "For a dataset with $ n $ observations, the total log-likelihood is the sum of the log-likelihoods for all individual observations:\n",
    "\n",
    "$$\n",
    "\\text{(Total) Log-Likelihood} = \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{p}(x_i)) + (1 - y_i) \\log(1 - \\hat{p}(x_i)) \\right]\n",
    "$$\n",
    "\n",
    "### 2.4 **Negative Log-Likelihood (Log-Loss)**:\n",
    "$$\n",
    "\\text{Negative Log-Likelihood} = -\\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{p}(x_i)) + (1 - y_i) \\log(1 - \\hat{p}(x_i)) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ y_i $ is the actual class label (0 or 1) for the $ i $-th observation.\n",
    "- $ \\hat{p}(x_i) $ is the estimated probability for the $ i $-th observation.\n",
    "\n",
    "Further:\n",
    "- The **Log-Loss** is the negative log-likelihood divided by the number of data points, $ n $, making it the average loss per observation.\n",
    "- It is commonly used in machine learning because it provides a normalized metric, allowing comparisons across datasets of different sizes.\n",
    "- Log-Loss is also known as **Cross-Entropy Loss** in the context of classification problems. When applied to binary classification, it is sometimes referred to as **Binary Cross-Entropy Loss** or **Binary Entropy Loss**.\n",
    "\n",
    "### 3. **Gradient Descent for Logistic Regression**\n",
    "\n",
    "Gradient Descent is used to minimize the log-loss and find the best estimated parameters $ \\hat{\\beta}_0 $ and $ \\hat{\\beta}_1 $.\n",
    "\n",
    "#### 3.1. **Compute the Gradients**\n",
    "\n",
    "The gradients of the log-loss with respect to $ \\hat{\\beta}_0 $ and $ \\hat{\\beta}_1 $ are:\n",
    "\n",
    "1. **Gradient with respect to $ \\hat{\\beta}_0 $**:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial \\text{Log-Loss}}{\\partial \\hat{\\beta}_0} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\hat{p}(x_i) - y_i \\right)\n",
    "   $$\n",
    "\n",
    "2. **Gradient with respect to $ \\hat{\\beta}_1 $**:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial \\text{Log-Loss}}{\\partial \\hat{\\beta}_1} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\hat{p}(x_i) - y_i \\right) x_i\n",
    "   $$\n",
    "\n",
    "#### 3.2. **Update Rules**\n",
    "\n",
    "Using the gradients, we update $ \\hat{\\beta}_0 $ and $ \\hat{\\beta}_1 $ using the following rules:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_0^{(k+1)} = \\hat{\\beta}_0^{(k)} - \\alpha \\frac{\\partial \\text{Log-Loss}}{\\partial \\hat{\\beta}_0}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_1^{(k+1)} = \\hat{\\beta}_1^{(k)} - \\alpha \\frac{\\partial \\text{Log-Loss}}{\\partial \\hat{\\beta}_1}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\hat{\\beta}_0^{(k)} $ and $ \\hat{\\beta}_1^{(k)} $ are the estimates at the $ k $-th iteration.\n",
    "- $ \\alpha $ is the learning rate.\n",
    "\n",
    "### 4. **Convergence and Final Model**\n",
    "\n",
    "The final model can then be used for making predictions:\n",
    "\n",
    "$$\n",
    "\\hat{p}(x) = \\frac{1}{1 + e^{10.65132823650608 - 0.005498915547165412 x}}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"images/11_logistic_regression_log_loss.PNG\" alt=\"Log-Loss\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. **Beyond Simple Logistic Regression Models**\n",
    "\n",
    "<img src=\"images/12_classification_two_independent_variables.PNG\" alt=\"2 Independent Variables\" width=\"1200\"/> <br>\n",
    "\n",
    "$$ \\hat{p}(y_i | x_{1i}, x_{2i}) = \\hat{p}(x) = \\frac{1}{1 + e^{-(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{1i} + \\hat{\\beta}_2 x_{2i})}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretability vs. Evaluation Metrics\n",
    "\n",
    "<img src=\"images/17_interpretability_vs_evaluation_metrics.PNG\" alt=\"Interpretability VS Evaluation Metrics\" width=\"1200\"/> <br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Neural Net\n",
    "<img src=\"images/18_ann.PNG\" alt=\"ANN\" width=\"1200\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning: clustering - in depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observed data\n",
    "\n",
    "| $(i)$ | Education $(x_{1i})$ | Seniority $(x_{2i})$ |\n",
    "|-------|----------------------|----------------------|\n",
    "| 1     | 19.9310344827586     | 168.965517241379     |\n",
    "| 2     | 20.3448275862069     | 187.586206896552     |\n",
    "| ...   | ...                  | ...                  |\n",
    "| n     | $(x_{1n})$           | $(x_{2n})$           |\n",
    "\n",
    "\n",
    "<br>\n",
    "<img src=\"images/13_unsupervised_data.PNG\" alt=\"K-Means Clustering Education Seniority Only Data\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **K-Means Clustering Model**\n",
    "\n",
    "The K-means clustering model aims to partition a dataset into $ k $ clusters, where each data point $ \\mathbf{x}_i $ is assigned to the cluster with the nearest centroid $ \\boldsymbol{\\mu}_j $. The model can be described by the following assignment rule:\n",
    "\n",
    "$$\n",
    "c_i = \\arg\\min_{j} \\|\\mathbf{x}_i - \\boldsymbol{\\mu}_j\\|^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ c_i $ is the index of the cluster assigned to data point $ \\mathbf{x}_i $.\n",
    "- $ \\|\\mathbf{x}_i - \\boldsymbol{\\mu}_j\\|^2 $ represents the squared Euclidean distance between $ \\mathbf{x}_i $ and centroid $ \\boldsymbol{\\mu}_j $.\n",
    "\n",
    "### 2. **Objective Function (Loss Function)**\n",
    "\n",
    "The **objective function** in K-means clustering is the **Within-Cluster Sum of Squares (WCSS)**, which measures the total variance within clusters. It quantifies how compact the clusters are by summing the squared Euclidean distances between each data point and its assigned cluster centroid. The objective function is defined as:\n",
    "\n",
    "$$\n",
    "\\text{WCSS} = \\sum_{i=1}^{n} \\sum_{j=1}^{k} \\mathbb{I}(c_i = j) \\|\\mathbf{x}_i - \\boldsymbol{\\mu}_j\\|^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ n $ is the number of data points.\n",
    "- $ k $ is the number of clusters.\n",
    "- $ \\mathbb{I}(c_i = j) $ is an indicator function that equals 1 if data point $ \\mathbf{x}_i $ is assigned to cluster $ j $ and 0 otherwise.\n",
    "- $ \\|\\mathbf{x}_i - \\boldsymbol{\\mu}_j\\|^2 $ is the squared Euclidean distance between data point $ \\mathbf{x}_i $ and centroid $ \\boldsymbol{\\mu}_j $.\n",
    "\n",
    "### 3. **Steps to Minimize the Objective Function**\n",
    "\n",
    "The K-means algorithm follows these steps:\n",
    "\n",
    "1. **Initialization**: (Each observation is randomly assigned to a cluster.) Randomly initialize $ k $ centroids $ \\boldsymbol{\\mu}_1, \\boldsymbol{\\mu}_2, \\ldots, \\boldsymbol{\\mu}_k $.\n",
    "\n",
    "2. **Assignment Step**: Assign each data point $ \\mathbf{x}_i $ to the nearest centroid:\n",
    "\n",
    "   $$\n",
    "   c_i = \\arg\\min_{j} \\|\\mathbf{x}_i - \\boldsymbol{\\mu}_j\\|^2\n",
    "   $$\n",
    "\n",
    "3. **Update Step**: Recalculate the centroids as the mean of all data points assigned to each cluster:\n",
    "\n",
    "   $$\n",
    "   \\boldsymbol{\\mu}_j = \\frac{1}{|C_j|} \\sum_{\\mathbf{x}_i \\in C_j} \\mathbf{x}_i\n",
    "   $$\n",
    "\n",
    "   where $ C_j $ is the set of data points assigned to cluster $ j $, and $ |C_j| $ is the number of points in cluster $ j $.\n",
    "\n",
    "4. **Repeat**: Continue iterating through the Assignment and Update steps until the centroids stabilize (i.e., their positions change very little) or until a maximum number of iterations is reached.\n",
    "\n",
    "<img src=\"images/14_K_means_clustering.PNG\" alt=\"K-Means Clustering Iteration\" width=\"1200\"/>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### 4. **Convergence and Final Model**\n",
    "\n",
    "The final model can then be used for making predictions:\n",
    "\n",
    "<img src=\"images/14_01_K_means_clustering_final_result.PNG\" alt=\"K-Means Clustering Final Result\" width=\"600\"/>\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"images/15_K_means_clustering_different_starting_values.PNG\" alt=\"K-Means Clustering Different Starting Values\" width=\"1200\"/>\n",
    "<br>\n",
    "WCSS values for different starting values and $K=3$, best solution is 235.8\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"images/16_K_means_clustering_elbow_plot.PNG\" alt=\"K-Means Clustering Elbow Plot\" width=\"1200\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Succeeding with Machine Learning goes beyond 'data is the new oil'\n",
    "\n",
    "- Business opportunity: \n",
    "    - Impact on P&L (revenue, costs).\n",
    "    - Customer-centric.\n",
    "    - Reach wide audience.\n",
    "    - Enhance user experience.\n",
    "- People: \n",
    "    - Right skills.\n",
    "    - Clear purpose / desire.\n",
    "    - Aligned with organization's goals.\n",
    "- Foundation model: \n",
    "    - Use it as-is.\n",
    "    - Fine-tune.\n",
    "- Data:\n",
    "    - Proprietary/unique data.\n",
    "    - Quality, up-to-date, ...\n",
    "- Act:\n",
    "    - MVP: Build > train > improve; iterate based on feedback.\n",
    "    - Measure impact (increased conversion rate by 20%, reduced customer churn by 15%, ...).\n",
    "- Resource commitment: Allocate sufficient resources, including time, budget, and personnel, to support ML projects.\n",
    "- Risks:\n",
    "    - Uncertainty: Significant upfront investment required, and unclear whether successful.\n",
    "    - Legal.\n",
    "    - Ethical.\n",
    "    - ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
